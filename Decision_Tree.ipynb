{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "\n",
        "\n",
        "*   **Decision Tree:-**\n",
        "    A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks, but in the context of classification, it predicts categorical outcomes by learning simple decision rules based on the features in the data.\n",
        "\n",
        "\n",
        "*    It Works for Classification:\n",
        "\n",
        "a. The process starts at the root node, where the data is partitioned based on the value of the most informative feature (using criteria like Gini impurity or information gain).\n",
        "\n",
        "b. At each node, the feature and threshold that best separates the classes are chosen, and the node branches to different sub-nodes or leaves depending on the feature value.\n",
        "\n",
        "c. This recursive splitting continues (using the divide-and-conquer strategy) until the data in each subset is as homogenous as possible regarding the target class—or until some stopping rules apply (such as maximum tree depth or minimum leaf size).\n",
        "\n",
        "d. The tree assigns a class label (or probability of classes) to new instances by routing them from the root down to a leaf, following the rules learned at each decision node.\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.How do they impact the splits in a Decision Tree?\n",
        "\n",
        "\n",
        "\n",
        "*  **Gini Impurity :-**\n",
        "\n",
        " a. Gini Impurity measures the probability of incorrectly classifying a randomly chosen sample from the node if that sample were randomly labeled based on the class distribution.\n",
        "\n",
        "b. For a node with k classes and class probabilities p1,p2,...,pk , the Gini Impurity is given by:\n",
        "\n",
        "                      Gini=1−∑i=1tok(pi)2\n",
        "\n",
        "c.  A Gini value of 0 indicates all samples in the node belong to a single class (pure node), while a higher Gini value indicates more class mixing (maximum value for binary class is 0.5).\n",
        "\n",
        "\n",
        "*  **Entropy:-**\n",
        "\n",
        "a. Entropy, in the context of Decision Trees, measures the level of impurity or disorder at a node. It represents the average amount of information (or surprise) required to classify a randomly picked instance in the node.\n",
        "\n",
        "b. For class probabilities p1,p2,..,pk , entropy is given by:\n",
        "\n",
        "               Entropy=−∑i=1tokpilog⁡2pi\n",
        "\n",
        "\n",
        "c. A node is pure (no disorder) when entropy is 0. Higher entropy indicates greater class mixing and increased uncertainty.\n",
        "\n",
        "*   Impact on Decision Tree Splits:\n",
        "\n",
        "a. Both Gini Impurity and Entropy are used to select the best feature for splitting by minimizing impurity (or maximizing the reduction in impurity).\n",
        "\n",
        "b.For each candidate split, the algorithm computes the weighted average impurity (using Gini or Entropy) for the child nodes, and selects the split that results in the lowest weighted impurity.\n",
        "\n",
        "c.While both measures tend to produce similar trees, Gini Impurity is often preferred for efficiency since it does not require logarithmic computations.\n",
        "\n",
        "d.The choice of impurity measure can slightly affect tree structure when splits are equally valid according to both criteria, but in practice the overall classification performance is usually similar.\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in DecisionTrees? Give one practical advantage of using each.\n",
        "\n",
        "\n",
        "\n",
        "*   **Pre-Pruning (Early Stopping):-**\n",
        "\n",
        "a.  Pre-pruning halts the growth of the decision tree during the training phase, stopping further splitting when a certain condition is met—such as maximum depth, minimum samples per leaf, or lack of further accuracy improvement.\n",
        "\n",
        "b. Advantage: A practical advantage is improved training efficiency and lower risk of overfitting on large datasets, since unnecessary branches are never created.\n",
        "*   **Post-Pruning (Reduced/Error Pruning):-**\n",
        "\n",
        "a. Post-pruning allows the tree to grow fully before examining the branches; non-significant nodes/branches are pruned back based on metrics like cross-validation accuracy, cost-complexity pruning, or error reduction.\n",
        "\n",
        "b. Advantage: A key benefit is enhanced -generalization—post-pruning often leads to higher accuracy on unseen data, especially with small or noisy datasets, because it rigorously evaluates which branches contribute predictive value.\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "a.**Information Gain (IG)** measures the reduction in entropy (uncertainty) of the target variable after splitting the data on a given feature.\n",
        "\n",
        "b. Formula:\n",
        "\n",
        "IG(D,A)=H(D)−H(D∣A)\n",
        "\n",
        "where:\n",
        "\n",
        "H(D) is the entropy of the original dataset D.\n",
        "\n",
        "H(D∣A) is the weighted average entropy of the dataset after splitting by feature A.\n",
        "\n",
        "c. The feature with the largest information gain is chosen for the split, as it produces the purest (most homogeneous) child nodes.\n",
        "\n",
        "Importance for Splits:-\n",
        "\n",
        " a .Information Gain identifies which feature provides the most \"explanatory power\" for classifying the samples at each node.\n",
        "\n",
        "b. By selecting the feature with the highest IG, the tree maximally reduces class impurity at each split, contributing to quick and effective separation of classes and improving overall classification performance.\n",
        "\n",
        "c.This mechanism is crucial for constructing efficient, accurate trees that avoid unnecessary complexity and overfitting.\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "**Common Real-World Applications**\n",
        "\n",
        "a.Credit Scoring & Risk Assessment: Used by banks and financial institutions to assess loan eligibility, evaluate credit risk, and segment borrowers based on attributes like credit history and income.\n",
        "\n",
        "b.Medical Diagnosis: Helps doctors diagnose diseases by analyzing patient symptoms, medical test results, and risk factors to guide treatment decisions.\n",
        "\n",
        "c.Fraud Detection: Flags suspicious transactions in finance by identifying deviations from typical user behavior or patterns.\n",
        "\n",
        "d.Customer Segmentation & Marketing: Segments customers by demographics or purchasing behavior to optimize targeted marketing campaigns and retention strategies.\n",
        "\n",
        "e.Manufacturing Quality Control: Predicts product defects and guides process improvements to reduce waste and maintain standards.\n",
        "\n",
        "f.E-commerce & Retail: Powers product recommendation engines, enables dynamic pricing, and helps manage inventory and promotions based on user data and purchase trends.\n",
        "\n",
        "g.Agriculture & Environmental Science: Predicts crop yields, optimizes planting schedules, and manages resources for precision farming.\n",
        "\n",
        "Main Advantages:-\n",
        "\n",
        "a.**Interpretability:** Decision Trees deliver outputs that are easy to visualize and interpret, enabling transparent decision-making and regulatory compliance.\n",
        "\n",
        "b.**Minimal Data Preparation:** Less sensitive to missing values and outliers, and often does not require complex feature scaling or transformation.\n",
        "\n",
        "c.**Handles Mixed Data Types:** Can process both categorical and numerical data without extra encoding.\n",
        "\n",
        "d. **Suitability for Nonlinear Relationships:** Can capture complex, nonlinear patterns in the data.\n",
        "\n",
        "Principal Limitations:-\n",
        "\n",
        "a.**Instability:** Small changes in data can lead to significant changes in the tree structure, making Decision Trees less stable than some other algorithms.\n",
        "\n",
        "b.**Overfitting:** Deep trees are prone to overfitting, especially when not properly pruned or regularized.\n",
        "\n",
        "c.**Lower Predictive Performance (Standalone):** Often outperformed by ensemble methods (e.g., Random Forests, Gradient Boosting) in terms of raw accuracy, especially on complex datasets.\n",
        "\n",
        "d.**Bias Toward Features with Many Categories:** Trees can be biased towards variables with numerous levels, sometimes leading to less meaningful splits.\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "● Print the model’s accuracy and feature importances.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cu10pMedhdl8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36egB2hLg7Jd",
        "outputId": "6f6fc3c6-8100-4b00-f86c-19dc3373b30f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 1.0000\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0179\n",
            "petal length (cm): 0.8997\n",
            "petal width (cm): 0.0824\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and test sets (75% train, 25% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Initialize Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on test data: {accuracy:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = clf.feature_importances_\n",
        "for feature_name, importance in zip(iris.feature_names, feature_importances):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree."
      ],
      "metadata": {
        "id": "kLCnz-ZkwHWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and test sets (75% training, 25% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "clf_max_depth = DecisionTreeClassifier(max_depth=3, criterion='gini', random_state=42)\n",
        "clf_max_depth.fit(X_train, y_train)\n",
        "y_pred_max_depth = clf_max_depth.predict(X_test)\n",
        "accuracy_max_depth = accuracy_score(y_test, y_pred_max_depth)\n",
        "\n",
        "# Train fully grown Decision Tree (no max_depth limit)\n",
        "clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print accuracies for comparison\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_max_depth:.4f}\")\n",
        "print(f\"Accuracy with fully grown tree: {accuracy_full:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljODXoA0wZ_G",
        "outputId": "9fc501bf-228d-40b1-d9f6-0f5f2e65802e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0000\n",
            "Accuracy with fully grown tree: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Load the California Housing dataset from sklearn\n",
        "\n",
        "● Train a Decision Tree Regressor\n",
        "\n",
        "● Print the Mean Squared Error (MSE) and feature importances."
      ],
      "metadata": {
        "id": "dh7dnZyJwu9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split dataset into training and test sets (75% train, 25% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Initialize Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the regressor\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE) on test data: {mse:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "for feature_name, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDXyCIekxP2J",
        "outputId": "0f887bf4-e698-472e-8740-73d936d83c9e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE) on test data: 0.5285\n",
            "MedInc: 0.5262\n",
            "HouseAge: 0.0509\n",
            "AveRooms: 0.0482\n",
            "AveBedrms: 0.0280\n",
            "Population: 0.0369\n",
            "AveOccup: 0.1349\n",
            "Latitude: 0.0880\n",
            "Longitude: 0.0868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "● Print the best parameters and the resulting model accuracy."
      ],
      "metadata": {
        "id": "Le9I6XQuxbVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning max_depth and min_samples_split\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV on training data to find the best parameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Use the best estimator to predict on the test data\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate and print test accuracy\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy of Best Model: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J2vZMvSxwcr",
        "outputId": "43ef5ece-22e2-49ec-9ff6-d8969af3a7ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Test Accuracy of Best Model: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "\n",
        "And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "1. Handle Missing Values\n",
        "Identify missing data patterns using data exploration techniques.\n",
        "\n",
        "For numerical features, impute missing values using median or mean imputation, as these are robust to outliers.\n",
        "\n",
        "For categorical features, impute with the most frequent category (mode) or create a special category like \"Unknown.\"\n",
        "\n",
        "Alternatively, use more advanced imputation like K-Nearest Neighbors or iterative imputation if the dataset and time allow.\n",
        "\n",
        "2. Encode Categorical Features\n",
        "Use One-Hot Encoding for nominal categorical variables to avoid ordinal assumptions.\n",
        "\n",
        "For high-cardinality categorical features, consider target encoding or frequency encoding carefully, ensuring no leakage in cross-validation.\n",
        "\n",
        "Ensure the encoding is consistent across training and test data (fit encoder on train, transform both sets).\n",
        "\n",
        "3. Train a Decision Tree Model\n",
        "Split the dataset into training and validation sets.\n",
        "\n",
        "Use a Decision Tree Classifier (e.g., from scikit-learn) since it naturally handles mixed data and categorical encodings well.\n",
        "\n",
        "Train the model on the training set using the Gini impurity or entropy criterion.\n",
        "\n",
        "4. Tune Hyperparameters\n",
        "Tune key hyperparameters such as max_depth, min_samples_split, and min_samples_leaf.\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV with cross-validation to reliably find the best hyperparameters.\n",
        "\n",
        "Monitor performance metrics relevant to healthcare, such as accuracy, precision, recall, and F1-score, during tuning.\n",
        "\n",
        "5. Evaluate Model Performance\n",
        "Evaluate final model on a hold-out test set using metrics aligned with clinical goals (e.g., high recall to avoid missing disease cases).\n",
        "\n",
        "Consider the confusion matrix to understand types of errors.\n",
        "\n",
        "Use AUC-ROC if probabilistic outputs are required.\n",
        "\n",
        "Business Value in Real-World Healthcare:-\n",
        "\n",
        "a.Early and accurate disease prediction can enable timely clinical interventions, improving patient outcomes.\n",
        "\n",
        "b.The model can assist healthcare providers by flagging high-risk patients for further testing and monitoring, optimizing resource allocation.\n",
        "\n",
        "c.Automating parts of diagnostic evaluation reduces human workload and potential for error.\n",
        "\n",
        "d.Understanding feature importances enhances transparency for clinicians, aiding trust and adoption.\n",
        "\n",
        "e.Ultimately, this predictive modeling supports personalized and proactive patient care, potentially reducing healthcare costs by preventing disease progression.\n",
        "\n"
      ],
      "metadata": {
        "id": "NC9dkMA2x9IW"
      }
    }
  ]
}